{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project - Stage 3\n",
    "\n",
    "## The ask\n",
    "\n",
    "\n",
    "\n",
    "## Programme Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import urllib.request\n",
    "from io import BytesIO\n",
    "from pdfminer3.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer3.converter import TextConverter\n",
    "from pdfminer3.layout import LAParams\n",
    "from pdfminer3.pdfpage import PDFPage\n",
    "import re\n",
    "import operator\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Text from remote PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get text from PDF document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PDF to text Function. \n",
    "def pdf_to_text(path):\n",
    "    '''\n",
    "        Returns list of text for each page. Length == # pages.\n",
    "    '''\n",
    "    manager = PDFResourceManager()\n",
    "    retstr = BytesIO()\n",
    "    layout = LAParams(all_texts=True)\n",
    "    device = TextConverter(manager, retstr, laparams=layout)\n",
    "    filepath = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(manager, device)\n",
    "    \n",
    "    text_list = []\n",
    "    for page in PDFPage.get_pages(filepath, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "        text_list.append(retstr.getvalue().decode('utf-8','ignore'))\n",
    "\n",
    "    filepath.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get PDF from remote URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch PDF from remote URL function\n",
    "def get_pdf_from_url(url,filename=None,print_text=False):\n",
    "    # Takes a URL and saves the data locally.\n",
    "    \n",
    "    if filename == None:\n",
    "        filename = url.split('/')[-1]\n",
    "    \n",
    "    webFile = urllib.request.urlopen(url)\n",
    "    with open(filename,'wb') as localFile:\n",
    "        contents = webFile.read()\n",
    "        localFile.write(contents)\n",
    "        \n",
    "    webFile.close()\n",
    "    \n",
    "    # get text from the pdf file\n",
    "    if print_text:\n",
    "        text = pdf_to_text(filename)\n",
    "        print(text)\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenise the data\n",
    "\n",
    "Extracting tokens from the data allows us to gain a better understanding and summarise the data.\n",
    "\n",
    "#### Different tokens we can have:\n",
    "    \n",
    "- words\n",
    "- phrases\n",
    "- sentences\n",
    "\n",
    "Previously, this function returned the chosen type of token, however, it may be better to add all types of tokens to a table that can be filtered as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenise\n",
    "def tokenise(text):\n",
    "    if type(text) == list:\n",
    "        text = ' '.join(text)\n",
    "    \n",
    "    # initiate pd DataFrame\n",
    "    tokens = pd.DataFrame(columns=['token','token_type','frequency'])\n",
    "    \n",
    "    # word tokens first\n",
    "    words_list = re.findall(\"[\\w']+\", text.lower())# return list of words\n",
    "    words_bot = bag_of_tokens(words_list)\n",
    "    words_df = pd.DataFrame(data={'token':list(words_bot.keys()),'token_type':'word','frequency':list(words_bot.values())})\n",
    "    tokens = tokens.append(words_df,ignore_index=True)\n",
    "    \n",
    "    sentences_list = re.compile('[.!?\\n]').split(text.lower())# return list of sentences\n",
    "    sentences_bot = bag_of_tokens(sentences_list)\n",
    "    sentences_df = pd.DataFrame(data={'token':list(sentences_bot.keys()),'token_type':'sentence','frequency':list(sentences_bot.values())})\n",
    "    tokens = tokens.append(sentences_df)\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words\n",
    "def bag_of_tokens(token_list,sort=True,reverse=False):\n",
    "    # Returns the frequencies of each token\n",
    "    bag_of_tokens = {} # dictionary that will contain each token and its frequency.\n",
    "    \n",
    "    for token in token_list:\n",
    "        token = token.strip(' \\t\\n\\r\\f')\n",
    "        if len(token) >= 1 and token != ' ':\n",
    "            if token in bag_of_tokens.keys():\n",
    "                bag_of_tokens[token] += 1\n",
    "            else:\n",
    "                bag_of_tokens[token] = 1\n",
    "    \n",
    "    if sort:\n",
    "        bag_of_tokens = dict(sorted(bag_of_tokens.items(), key=operator.itemgetter(1),reverse=reverse))\n",
    "\n",
    "    return bag_of_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf'\n",
    "url = 'http://www.africau.edu/images/default/sample.pdf'\n",
    "filename = get_pdf_from_url(url)\n",
    "pages = pdf_to_text(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>token_type</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>more</td>\n",
       "      <td>word</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>text</td>\n",
       "      <td>word</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>and</td>\n",
       "      <td>word</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>and more text</td>\n",
       "      <td>sentence</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>text</td>\n",
       "      <td>sentence</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>and more</td>\n",
       "      <td>sentence</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>pdf</td>\n",
       "      <td>word</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>a</td>\n",
       "      <td>word</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>file</td>\n",
       "      <td>word</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>boring</td>\n",
       "      <td>word</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            token token_type frequency\n",
       "41           more       word        70\n",
       "40           text       word        67\n",
       "39            and       word        64\n",
       "20  and more text   sentence        54\n",
       "19           text   sentence         7\n",
       "18       and more   sentence         7\n",
       "36            pdf       word         5\n",
       "35              a       word         5\n",
       "37           file       word         5\n",
       "38         boring       word         5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenise(pages)\n",
    "tokens.sort_values('frequency',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further pre-processing\n",
    "\n",
    "Additional to extracting the text from the document and forming a structured table of various types of tokens, other techniques can be applied to reduce the variance in the specific words seen while paying more attention to the meaning behind the text.\n",
    "\n",
    "Such techniques are, 'Stemming' and 'Lemmatisation'. The ideas behind them is to reduce each word to their root meaning. This is done in the following ways...\n",
    "\n",
    "- <b>Stemming</b>\n",
    "\n",
    "    Stemming algorithms are a set of specific rules and actions that are applied to reduce the word to its root. This method does not always return a meaning full word in the relevant language, however, the root should be the same for similar words.\n",
    "\n",
    "    The most common algorithm for the English language is \"Porter's Algorithm\". This algorithm is very complex but it essentially comprises of 5 phases of word reductions, applied sequentially.\n",
    "    \n",
    "\n",
    "- <b>Lemmatisation</b>\n",
    "\n",
    "    This method involves performing lookups for the word, against a database that maps similar words together. The result of this technique are the base, or dictionary representation, of the word; known as the 'lemma'.\n",
    "\n",
    "[(Stemming and Lemmatisation - Standford University)](https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)\n",
    "\n",
    "#### Remove high frequency words\n",
    "\n",
    "It is also common to reduce high frequency words that often don't contribute to the underlying meaning. There are various ways to do this, which are subject to the aim of the project in hand. The most common practise is to remove 'stop-words' such as \"and\", \"a\", \"the\" etc. These are words that are required for the text to make sense linguistically, but contribute very little to the actual meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words\n",
    "Makes use of NLTK library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(token_df):\n",
    "    for index, row in token_df.iterrows():\n",
    "        if row.token in stopwords.words('english'):\n",
    "            token_df.drop(index,inplace=True)\n",
    "    return token_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>token_type</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>text</td>\n",
       "      <td>word</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>text</td>\n",
       "      <td>sentence</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>boring</td>\n",
       "      <td>word</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>file</td>\n",
       "      <td>word</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>pdf</td>\n",
       "      <td>word</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>more text</td>\n",
       "      <td>sentence</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>simple</td>\n",
       "      <td>word</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2</td>\n",
       "      <td>word</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>page</td>\n",
       "      <td>word</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>continued</td>\n",
       "      <td>word</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        token token_type frequency\n",
       "40       text       word        67\n",
       "19       text   sentence         7\n",
       "38     boring       word         5\n",
       "37       file       word         5\n",
       "36        pdf       word         5\n",
       "17  more text   sentence         4\n",
       "27     simple       word         3\n",
       "33          2       word         3\n",
       "32       page       word         3\n",
       "31  continued       word         3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "without_stops = remove_stop_words(tokens)\n",
    "without_stops.sort_values('frequency',ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatisation\n",
    "\n",
    "Makes use of NLTK library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def porter_stem(token_df):\n",
    "    porter = PorterStemmer()\n",
    "    stems = token_df[0:0]\n",
    "    for index, row in token_df.iterrows():\n",
    "        if row.token_type in ['sentence','phrase']:\n",
    "            stem = ''\n",
    "            for word in row.token.split(' '):\n",
    "                stem += ' ' + porter.stem(word)\n",
    "            stem = stem.strip()\n",
    "        else:\n",
    "            stem = porter.stem(row.token)\n",
    "        if stem in stems.token:\n",
    "            stems.at[stems[stems.token==stem].index[0],'frequency'] += row.frequency\n",
    "        else:\n",
    "            stems = stems.append({'token':stem,'token_type':row.token_type,'frequency':row.frequency},ignore_index=True)\n",
    "    return stems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before applying the POS (Part-of-Speech) tag, some words, such as, 'watching' would be lemmatised to 'watching' - clearly this is not useful.\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def lemmatise(token_df):\n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "    stems = token_df[0:0]\n",
    "    for index, row in token_df.iterrows():\n",
    "        if row.token_type in ['sentence','phrase']:\n",
    "            stem = ''\n",
    "            for word in row.token.split(' '):\n",
    "                stem += ' ' + lemmatiser.lemmatize(word,get_wordnet_pos(word))\n",
    "            stem = stem.strip()\n",
    "        else:\n",
    "            stem = lemmatiser.lemmatize(row.token,get_wordnet_pos(row.token))\n",
    "        if stem in stems.token:\n",
    "            stems.at[stems[stems.token==stem].index[0],'frequency'] += row.frequency\n",
    "        else:\n",
    "            stems = stems.append({'token':stem,'token_type':row.token_type,'frequency':row.frequency},ignore_index=True)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>token_type</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>word</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yet</td>\n",
       "      <td>word</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oh</td>\n",
       "      <td>word</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type</td>\n",
       "      <td>word</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stuff</td>\n",
       "      <td>word</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>watch</td>\n",
       "      <td>word</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>paint</td>\n",
       "      <td>word</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dri</td>\n",
       "      <td>word</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>littl</td>\n",
       "      <td>word</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>end</td>\n",
       "      <td>word</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   token token_type frequency\n",
       "0      1       word         1\n",
       "1    yet       word         1\n",
       "2     oh       word         1\n",
       "3   type       word         1\n",
       "4  stuff       word         1\n",
       "5  watch       word         1\n",
       "6  paint       word         1\n",
       "7    dri       word         1\n",
       "8  littl       word         1\n",
       "9    end       word         1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pstems = porter_stem(tokens)\n",
    "lstems = lemmatise(tokens)\n",
    "pstems.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>token_type</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>word</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yet</td>\n",
       "      <td>word</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oh</td>\n",
       "      <td>word</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>type</td>\n",
       "      <td>word</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>stuff</td>\n",
       "      <td>word</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>watch</td>\n",
       "      <td>word</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>paint</td>\n",
       "      <td>word</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dry</td>\n",
       "      <td>word</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>little</td>\n",
       "      <td>word</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>end</td>\n",
       "      <td>word</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    token token_type frequency\n",
       "0       1       word         1\n",
       "1     yet       word         1\n",
       "2      oh       word         1\n",
       "3    type       word         1\n",
       "4   stuff       word         1\n",
       "5   watch       word         1\n",
       "6   paint       word         1\n",
       "7     dry       word         1\n",
       "8  little       word         1\n",
       "9     end       word         1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstems.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "The above two stemming methods, on the whole return similar output. However, it is worth noting that in experimentation, words such as 'caring' will return 'car' when using Porter Stemming and 'care' when using Lemmatisation. Clearly the latter is the correct result. In this case, Porter Stemming actually risks changing the meaning of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' A Simple PDF File \\n\\n This is a small demonstration .pdf file - \\n\\n just for use in the Virtual Mechanics tutorials. More text. And more \\n text. And more text. And more text. And more text. \\n\\n And more text. And more text. And more text. And more text. And more \\n text. And more text. Boring, zzzzz. And more text. And more text. And \\n more text. And more text. And more text. And more text. And more text. \\n And more text. And more text. \\n\\n And more text. And more text. And more text. And more text. And more \\n text. And more text. And more text. Even more. Continued on page 2 ...\\n\\n\\x0c'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Simple PDF File \n",
      "\n",
      "  ORG\n",
      "2 CARDINAL\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = nlp(pages[0])\n",
    "for ent in text.ents:\n",
    "    print(ent.text,ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"> \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    A Simple PDF File \n",
       "\n",
       " \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "This is a small demonstration .pdf file - </br></br> just for use in the Virtual Mechanics tutorials. More text. And more </br> text. And more text. And more text. And more text. </br></br> And more text. And more text. And more text. And more text. And more </br> text. And more text. Boring, zzzzz. And more text. And more text. And </br> more text. And more text. And more text. And more text. And more text. </br> And more text. And more text. </br></br> And more text. And more text. And more text. And more text. And more </br> text. And more text. And more text. Even more. Continued on page \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " ...\n",
       "\n",
       "\f",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(text,style=\"ent\",jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
